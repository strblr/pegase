{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"\u26a0\ufe0f This library is still under development. This is a pre-release but some functionalities might still change. Overview Pegase is a PEG parser generator for JavaScript and TypeScript. It's: Inline , meaning grammars are directly expressed as tagged template literals . No generation step, no CLI. Pegase works in symbiosis with JS. Fast . Pegase is heavily optimized to be extremely fast while providing an extensive range of features. Complete . Pegase has everything you will ever need: an elegant grammar syntax with a lot of flexibility, semantic actions, parametrized rules, support for native regexps, error recovery, warnings, AST generation, AST visitors, cut operator, back references, and a lot more. Lightweight . Pegase is a zero-dependency package, and weights around 8kB gzipped. Intuitive , in that it lets you express complex grammars and semantic processes in very simple ways. You will never feel lost. Extensible : You can define your own Parser subclasses, add plugins, write custom directives, etc. Motivation The first and main goal of this library is to get you quickly and painlessly into parsing. Let's take a look at an example: parsing math expressions. With very few lines of code, some directives and semantic actions, you already have a parser and a calculator : import peg from \"pegase\"; function calc(left, op, right) { switch (op) { case \"+\": return left + right; case \"-\": return left - right; case \"*\": return left * right; case \"/\": return left / right; } } const expr = peg` expr: term % (\"+\" | \"-\") @infix(${calc}) term: fact % (\"*\" | \"/\") @infix(${calc}) fact: $integer | '(' expr ')' $integer @number: '-'? [0-9]+ `; Let's see how this plays out : expr.value(\"2 + (17-2*30) *(-5)+2\") 219 expr.test(\"2* (4 + )/32\") false expr.parse(\"2* (4 + )/32\").logger.toString() (1:9) Failure: Expected integer or \"(\" > 1 | 2* (4 + )/32 | ^ A few early notes here : a % b is a shortcut for a (b a)* , meaning \"any sequence of a separated by b \" . Think of parsers as black boxes emitting (if they succeed) zero or more values, called children . These boxes can be composed together to form more complex parsers. @infix is a directive. Directives are functions that transform a parser into another, usually to add some desired behavior. By default, whitespace skipping is automatically handled for you. It's of course entirely configurable. Rules starting with $ are tokens . Tokens are parsers with special behavior regarding failure reporting and whitespace skipping. Notice how some literals are single-quoted like ')' or double-quoted like \"+\" . Double-quote literals emit their string match as a single child, while single-quotes are silent. Writing the operators with double quotes allows them to be accumulated and processed with @infix . Don't worry if things aren't so clear yet. The rest of the documentation below is here to go step by step in all the underlying concepts, so that you understand the core philosophy and principles at hand. Try-it out You can try everything out while reading this website by accessing the JS console tab. The peg tag and hooks will be directly available in your namespace. All other named exports from pegase are available as properties of _ . Have fun!","title":"Home"},{"location":"#overview","text":"Pegase is a PEG parser generator for JavaScript and TypeScript. It's: Inline , meaning grammars are directly expressed as tagged template literals . No generation step, no CLI. Pegase works in symbiosis with JS. Fast . Pegase is heavily optimized to be extremely fast while providing an extensive range of features. Complete . Pegase has everything you will ever need: an elegant grammar syntax with a lot of flexibility, semantic actions, parametrized rules, support for native regexps, error recovery, warnings, AST generation, AST visitors, cut operator, back references, and a lot more. Lightweight . Pegase is a zero-dependency package, and weights around 8kB gzipped. Intuitive , in that it lets you express complex grammars and semantic processes in very simple ways. You will never feel lost. Extensible : You can define your own Parser subclasses, add plugins, write custom directives, etc.","title":"Overview"},{"location":"#motivation","text":"The first and main goal of this library is to get you quickly and painlessly into parsing. Let's take a look at an example: parsing math expressions. With very few lines of code, some directives and semantic actions, you already have a parser and a calculator : import peg from \"pegase\"; function calc(left, op, right) { switch (op) { case \"+\": return left + right; case \"-\": return left - right; case \"*\": return left * right; case \"/\": return left / right; } } const expr = peg` expr: term % (\"+\" | \"-\") @infix(${calc}) term: fact % (\"*\" | \"/\") @infix(${calc}) fact: $integer | '(' expr ')' $integer @number: '-'? [0-9]+ `; Let's see how this plays out : expr.value(\"2 + (17-2*30) *(-5)+2\") 219 expr.test(\"2* (4 + )/32\") false expr.parse(\"2* (4 + )/32\").logger.toString() (1:9) Failure: Expected integer or \"(\" > 1 | 2* (4 + )/32 | ^ A few early notes here : a % b is a shortcut for a (b a)* , meaning \"any sequence of a separated by b \" . Think of parsers as black boxes emitting (if they succeed) zero or more values, called children . These boxes can be composed together to form more complex parsers. @infix is a directive. Directives are functions that transform a parser into another, usually to add some desired behavior. By default, whitespace skipping is automatically handled for you. It's of course entirely configurable. Rules starting with $ are tokens . Tokens are parsers with special behavior regarding failure reporting and whitespace skipping. Notice how some literals are single-quoted like ')' or double-quoted like \"+\" . Double-quote literals emit their string match as a single child, while single-quotes are silent. Writing the operators with double quotes allows them to be accumulated and processed with @infix . Don't worry if things aren't so clear yet. The rest of the documentation below is here to go step by step in all the underlying concepts, so that you understand the core philosophy and principles at hand.","title":"Motivation"},{"location":"#try-it-out","text":"You can try everything out while reading this website by accessing the JS console tab. The peg tag and hooks will be directly available in your namespace. All other named exports from pegase are available as properties of _ . Have fun!","title":"Try-it out"},{"location":"Quick-start/","text":"First, add Pegase as a dependency: npm install pegase or yarn add pegase Next, import the template literal tag that will become your new best friend and there you go, ready to write your first peg expression. import peg from \"pegase\"; const parser = peg`your peg expression`; What about a parser that recognizes a binary digit ? That's a simple alternative: const bit = peg`0 | 1`; Ok, bit is now a Parser instance, which has 4 methods : parse , test , value and children . Let's take a look at test . It takes a string input and returns true or false (whether the string conforms to the pattern or not). if (bit.test(\"1\")) console.log(\"It's a match!\"); What about an array of bits like [0, 1, 1, 0, 1] ? const bitArray = peg`'[' (0 | 1) % ',' ']'`; The % operator can be read as \"separated by\". Let's test it: if (bitArray.test(\" [ 0,1, 1 ,0 , 1 ] \")) console.log(\"It's a match!\"); As you might have spotted, whitespaces are handled automatically by default ( it can be changed ). The way this works is pretty simple: whitespace characters are parsed and discarded before every terminal parser (like '[' , 1 , etc.). This process is called skipping . By default, every parser also adds an implicit \"end of input\" symbol ( $ ) at the end of the peg expression, which is a terminal, thus the trailing space is skipped too and the whole string matches. Good, but so far, a RegExp could have done the job. Things get interesting when we add in non-terminals . A non-terminal is an identifier that refers to a more complex peg expression which will be invoked every time the identifier is used. You can think of non-terminals as variables whose value is a parser, initialized in what we call rules . This allows for recursive patterns. Let's say we want to match possibly infinitely-nested bit arrays: const nestedBitArray = peg` bitArray: '[' (bit | bitArray) % ',' ']' bit: 0 | 1 `; We have two rules: bitArray and bit . A collection of rules is called a grammar . The generated parser, nestedBitArray , always points to the topmost rule, bitArray in this case. Testing it: nestedBitArray.test(\"[[0]\"); // false nestedBitArray.test(\"[ [1, 0], 1] \"); // true nestedBitArray.test(\" [0, [[0] ]]\"); // true If we already defined bit as a JS variable, we're not obligated to redefine it as a rule. We can simply inject it as a tag argument: const bit = peg`0 | 1`; const nestedBitArray = peg` bitArray: '[' (${bit} | bitArray) % ',' ']' `; Okay, the test method is fun but what if you want to do something more elaborated like collecting values, reading warnings or parse failures ? The parse method is what you're asking for. It returns a result object containing all the info you might be interested in after a parsing. In fact, all other Parser methods ( test , value and children ) are wrappers around parse . const result = nestedBitArray.parse(\"[[0]\"); if(!result.success) console.log(result.logger.toString()); This will output: (1:5) Failure: Expected \",\" or \"]\" > 1 | [[0] | ^ What we are going to do next is collecting the bits we matched in an array. Every parser and subparser has the ability to emit values on success. These values are called children and can be processed in parent parsers, which in turn emit children , etc. You can think of children as Pegase's version of synthesized attributes , values that bubble from bottom to top. Back to the grammar. By writing \"0\" instead of 0 or '0' , it will emit the matched substring as a single child (same for 1 ): const bit = peg`\"0\" | \"1\"`; bit.parse(\"1\").children; // [\"1\"] Or directly: bit.children(\"1\"); // [\"1\"] children are automatically concatenated in case of sequence and repetition: const bitArray = peg`'[' (\"0\" | \"1\") % ',' ']'`; bitArray.children(\"[0, 1, 1, 0, 1]\"); // [\"0\", \"1\", \"1\", \"0\", \"1\"] You can wrap any peg expression in functions inserted via tag argument. These functions are called semantic actions . Actions can, among many other things, read their subparser's children , and process them. Let's wrap our entire expression in an action and console.log the children from there: import peg, { $children } from \"pegase\"; const bitArray = peg` '[' (\"0\" | \"1\") % ',' ']' ${() => console.log($children())} `; bitArray.parse(\"[0, 1, 1, 0, 1]\"); // console.log: [\"0\", \"1\", \"1\", \"0\", \"1\"] If we return a value in our semantic action, it will be emitted as a single child in replacement of the previous children . Let's use this to sum our bits: const bitArray = peg` '[' (\"0\" | \"1\") % ',' ']' ${() => $children().reduce((a, b) => a + Number(b), 0)} `; bitArray.children(\"[0, 1, 1, 0, 1]\"); // [3] When a parser emits a single child, that child is said to be the value of the parser: bitArray.value(\"[0, 1, 1, 0, 1]\"); // 3 Some behaviors are so commonly used that they are abstracted away in reusable bricks called directives . Similarly to semantic actions, un directive wraps a parser and produces another parser. Here is an example of a standard directive, @reverse , that... well, reverses the children : const bitArray = peg`'[' (\"0\" | \"1\") % ',' ']' @reverse`; bitArray.children(\"[0, 1, 1, 0, 1]\"); // [\"1\", \"0\", \"1\", \"1\", \"0\"]","title":"Quick start"},{"location":"advanced-concepts/AST-and-visitors/","text":"We saw in Basic concepts > Semantic actions and dataflow that a parsing process can be represented as an invocation tree, called concrete syntax tree . This tree doesn't actually exist except temporarily in the JS call stack, thus semantic processes you want to fire at some \"nodes\" have to be executed at parse time. This is what semantic actions are for. You can do a lot with that, but it might not always be sufficient nor practical. For example, most real-life compilers do several traversals of the syntax tree, some dependent on the previous ones, with a clear separation of concerns. For the tree to be traversed multiple times, it has to be generated and kept in memory. You generally don't want to generate the whole concrete syntax tree which might have lots of parts only relevant to the syntax analysis but irrelevant in later stages. The actual tree you care about has custom nodes and is called abstract syntax tree . Pegase provides a clean and elegant way to generate ASTs: the $node hook. This hook can be called from semantic actions and has the following signature: (label: string, fields: Record<string, any>) => Node Given a label to distinguish between node types and some custom fields, it builds and returns a Node object with the following signature: type Node = { $label: string; $from: Location; $to: Location; [field: string]: any; } The $label field and the custom fields simply correspond to $node 's arguments. The $from and $to keys are automatically set and indicate the boundaries of the match where the node was produced. Nodes can then be emitted, propagated and captured during the parsing process just like any children . The specifics of how this happens is totally up to you (see Basic concepts > Semantic actions and dataflow ). Here is an example of a grammar generating an AST for sums written in prefix notation: const prefix = peg` expr: | <>integer ${({ integer }) => $node(\"INT\", { integer })} | '+' <a>expr <b>expr ${({ a, b }) => $node(\"PLUS\", { a, b })} $integer @raw: \\d+ `; const ast = prefix.value(\"+ 12 + 42 3\"); The resulting ast will look like this: You may have noticed that the custom node fields are the captures. This is actually a very common practice and Pegase offers a shortcut for it: the standard @node directive. This directive takes the node label as an argument, and automatically emits a Node whose custom fields are the captures. In other words, the following is strictly equivalent to the previous example: const prefix = peg` expr: | <>integer @node('INT') | '+' <a>expr <b>expr @node('PLUS') $integer @raw: \\d+ `; But it gets even better. Just like $ rules are syntactic sugar for @token rules, the => operator is syntactic sugar for @node : const prefix = peg` expr: | <>integer => 'INT' | '+' <a>expr <b>expr => 'PLUS' $integer @raw: \\d+ `; What if you still want to tweak some fields before setting up the Node ? Well, the @node directive can take as a second argument a function that maps the captures to custom fields. These fields will be merged to the existing captures and the result will be set as the Node 's custom fields. It gives you the flexibility of an explicit call to the $node hook, with the brevity and expressivity of the directive. To illustrate that, let's parse complex numbers written in the form \"a + bi\" . The imaginary part is optional, so the i capture may be undefined . If that's indeed the case, we want our i node field to default to zero: const complex = peg` complex: <r>num <i>('+' num 'i')? @node('COMPLEX', ${({ i }) => ({ i: i || 0 })}) $num @number: \\d+ `; complex.value(\"13+6i\"); // { $label: \"COMPLEX\", $from: (...), $to: (...), r: 13, i: 6 } complex.value(\"42\"); // { $label: \"COMPLEX\", $from: (...), $to: (...), r: 42, i: 0 } Once an AST is generated, the next step is obviously to implement traversal procedures. The possibilities are nearly infinite: performing semantic checks (like type checks), fine-tuning a syntactic analysis, mutating the tree (like Babel plugins ), folding the tree to some output value, etc. To implement traversals of ASTs, Pegase ships with its own visitor pattern . A Pegase visitor is an object whose keys are node labels and whose values are callbacks taking a Node as single argument: type Visitor = { [label: string]: (node: Node) => any } The result of a visitor for a given node n is the return value of the callback associated with the label of n . Visitors are directly passed via the visit option to a parser's parse , test , value or children method, either as a single visitor or as an array of visitors forming a visitor pipe. After the parsing is done, the final children array will be mapped through the visitor pipe. Every children item will individually be sent down the visitor pipe. Each visitor feeds its result to next one. The result of the final visitor will replace the initial child. This mechanism implies two things: children never changes size as a result of visits, it's just a one-to-one mapping. Thus parsers who produce a value (ie. a single child) keep producing a value no matter how many visitors you stack. The final value will be the result of the visitor pipe applied to the initial value . Only the last visitor can return a non- Node result, since each visitor has to be fed with a Node value. Let's build a simple visitor that transforms the output Node of our previous prefix grammar into its label: const prefix = peg` expr: | <>integer => 'INT' | '+' <a>expr <b>expr => 'PLUS' $integer @raw: \\d+ `; const labelVisitor = { INT: node => node.$label, PLUS: node => node.$label }; prefix.value(\"182\", { visit: labelVisitor }); // \"INT\" prefix.value(\"+ 12 + 42 3\", { visit: labelVisitor }); // \"PLUS\" As you might have spotted, the INT and PLUS callbacks are exactly the same. You can replace them with a simple default case callback by using the $default key. This callback will be called when no other visitor key matches the current node label. const labelVisitor = { $default: node => node.$label }; Let's dive a bit deeper: how would you implement a visitor that calculates the sum's result ? Calculating a sum from an AST means folding the AST into a single value, which implies recursive visits to child nodes. That's exactly what the $visit hook is for: called from inside a visitor callback and given a node, it applies the current visitor to that node and returns the result. Our sum visitor could be implemented as follows: const sumVisitor = { INT: node => Number(node.integer), PLUS: node => $visit(node.a) + $visit(node.b) }; prefix.value(\"182\", { visit: sumVisitor }); // 182 prefix.value(\"+ 12 + 42 3\", { visit: sumVisitor }); // 57 Next, to illustrate visitor piping, we're going to add a visitor right before sumVisitor that preserves the AST but doubles the integer value of INT nodes. This basically implies that each visitor callback will have to be an identity function, returning the node it was passed and only performing side-effects. For INT nodes, the side-effect is to double the value. For PLUS nodes, it's to visit the child nodes. Giving us: const doubleVisitor = { INT: node => { node.integer *= 2; return node; }, PLUS: node => { $visit(node.a); $visit(node.b); return node; } }; prefix.value(\"182\", { visit: [doubleVisitor, sumVisitor] }); // 364 prefix.value(\"+ 12 + 42 3\", { visit: [doubleVisitor, sumVisitor] }); // 114 You get the idea. Have fun ! A visitor callback has access to all the hooks available in semantic actions , except $children , $value , $commit and $emit . So it's totally fine to emit warnings and failures from visitors: const sumVisitor = { INT: node => { if (node.integer === \"42\") $warn(\"42 is too powerful\"); return Number(node.integer); }, PLUS: node => $visit(node.a) + $visit(node.b) }; prefix.parse(\"+ 12 + 42 3\", { visit: sumVisitor }).logger.toString() (1:8) Warning: 42 is too powerful > 1 | + 12 + 42 3 | ^ The effect of some hooks differs when used in a semantic action vs. a visitor. In semantic actions, $fail and $expected don't commit failures, they emit failure candidates which are then merged or filtered out using the farthest failure heuristic (see Basic concepts > Failures and warnings ). In visitors, these hooks commit failures directly. The heuristic wouldn't make much sense outside of a backtracking syntactic analysis. Please refer to API > Hooks for an exhaustive doc of all hooks. If you need to separate the parsing and the visits (for example to pass different contexts to different visitors), you can make an explicit call to the applyVisitor utility function: const result = prefix.parse(\"+ 12 + 42 3\"); if(result.success) { result.options.context = newContext; // Changing context const value = applyVisitor(result.value, sumVisitor, result.options); // Explicit visit // ... } See API > Utility functions .","title":"AST and visitors"},{"location":"advanced-concepts/Cut-operator/","text":"Pegase implements the concept of cut points in the form of a cut operator: ^ . Sometimes when passing a certain point in an ordered choice expression (or alternative expression ), you know for sure that every remaining options would fail. That \"point\" can be marked explicitly by ^ in your peg expression and has the effect to commit to the current alternative . Thus, even if it were to fail afterwards, the other alternatives would not be tried out. In other words, the cut operator prevents local backtracking which can be a huge performance booster. Let's say you want to write a compiler for a C-like language. You define an instr rule that can match an if statement, a while loop or a do...while loop. If the terminal 'if' successfully matched, then even if the rest of the expression fails, there is just no way for a while loop or a do...while loop to match. It means that you can insert a cut point right after 'if' . The same reasoning can be applied to the 'while' terminal, but is useless for 'do' since it's already the last alternative. const p = peg` instr: | 'if' ^ '(' expr ')' instr | 'while' ^ '(' expr ')' instr | 'do' instr 'while' '(' expr ')' `; Since ^ is implemented as a no-op Parser (always succeeding and nothing is consumed nor emitted), it can also be used to implement default cases in alternative expressions. It differs from the empty literal parser in that the latter might skip whitespaces. const p = peg` size: | 'small' ${() => new SmallSize()} | 'big' ${() => new BigSize()} | ^ ${() => new DefaultSize()} `; Used outside an ordered choice expression, it's simply a no-op. The place it appears inside the ordered choice doesn't matter. The only thing that would override its effect is another ordered choice expression. Taking it to the extreme, this means that you may use it in a parameter expression of a non-terminal invocation. It would still apply for the ordered choice expression in which the non-terminal is called: const p = peg` expr: zee('x' ^ 'y') | zee('u') zee(prefix): prefix 'z' ` If \"x\" is matched, zee('u') would never be tried out.","title":"Cut operator"},{"location":"advanced-concepts/Debugging-with-tracers/","text":"When writing complex grammars and not getting the expected behavior, it's not always clear what's going on. Pegase gives you a tool to debug your grammars: tracers . A tracer is a function that will be called when entering a non-terminal, successfully matching a non-terminal, or failing to match it. Tracing is configured via two parse options: trace , a boolean to activate or deactivate tracing. This option can be toggled only for specific sections of the grammar by using the @trace and @notrace standard directives. See defaultPlugin . tracer , a function that takes a trace event ( enter , match , or fail ) as a single argument. A trace event is a simple object with contextual information. Please refer to API > Types to see the precise signature of these objects. If tracer is omitted, a default tracer will be provided that will pretty-print the trace events to console.log automatically. Let's illustrate that: const g = peg` array: '[' ($number | $boolean) % ',' ']' $number: \\d+ $boolean: 'true' | 'false' `; g.parse(\"[12, true]\", { trace: true }); This will produce the following console.log output: Entered \"array\" at (1:1) Entered \"number\" at (1:2) Matched \"number\" from (1:2) to (1:4) Entered \"number\" at (1:5) Failed \"number\" at (1:5) Entered \"boolean\" at (1:5) Matched \"boolean\" from (1:6) to (1:10) Matched \"array\" from (1:1) to (1:11) If the default tracer doesn't suit your needs, pass a custom one to the tracer option. Here is an example of a custom tracer that redirects trace events to an EventEmitter in a node environment: const emitter = new EventEmitter(); g.parse(\"[12, true]\", { trace: true, tracer: event => emitter.emit(\"traced\", event) });","title":"Debugging with tracers"},{"location":"advanced-concepts/Error-recovery/","text":"Sometimes you may need to keep parsing even after encountering what would otherwise be a fatal input error. This is the case in various compilers to report as many errors and warnings as possible in a single pass. It's also common practice in most advanced code editors in order to provide correct syntax highlighting and autocompletion even when you're not done typing. The way Pegase works without error recovery is described in Basic concepts > Failures and warnings : failures may be an instruction to backtrack or the sign of an actual input error. That's why emitted failures are not directly collected into an array, they are emitted as candidates and sorted out using the farthest failure heuristic. The general idea of error recovery is to commit the current farthest failure to the final failures array, and resume parsing after skipping the erroneous input section. Here is how it's done with Pegase: Identify the subsection of your peg expression that you wanna recover from. Add an alternative to this subsection. In it: Commit the current farthest failure by using the $commit hook or the @commit directive. Identify an expression / terminal / set of terminals you're likely to find after the erroneous portion and from where the parsing can resume from . This is called a synchronization expression . In C-like languages, it could be the semi-colon ; or a closing block bracket } for example. Skip input character by character until that expression is found. This is called synchronization and can be achieved by using the ... operator, called sync operator. The peg expression ...a is in fact syntactic sugar for (!a .)* a . See Basic concepts > Building parsers . Here is a grammar that parses an array of bits: const g = peg` bitArray: '[' bit % ',' ']' bit: 0 | 1 `; Given a erroneous input like [0, 4, 1, 2, 0, 1] , the parser won't be able to parse past 4 : g.parse(\"[0, 4, 1, 2, 0, 1]\").logger.toString() (1:5) Failure: Expected \"0\" or \"1\" > 1 | [0, 4, 1, 2, 0, 1] | ^ If we want to report every faulty bit, we need to add an alternative to bit where we'll commit the current farthest failure and sync the parser. To do the latter, we need to identify a synchronization expression. The question to ask here is: \"What can follow a bit ?\" . The answer is ',' or ']' . Which gives us: const g = peg` bitArray: '[' (bit | sync) % ',' ']' bit: 0 | 1 sync: @@commit ...&(',' | ']') `; g.parse(\"[0, 4, 1, 2, 0, 1]\").logger.toString() (1:5) Failure: Expected \"0\" or \"1\" > 1 | [0, 4, 1, 2, 0, 1] | ^ (1:11) Failure: Expected \"0\" or \"1\" > 1 | [0, 4, 1, 2, 0, 1] | ^ Be aware : the success status of the parsing will be true . With error recovery, a successful parsing doesn't necessarily imply \"no failure\", it just tells you that the parsing was able to finish successfully. This is indeed what recovery means. To check if there are any failures, check the size of the failures array: g.parse(\"[1, 0, 1, 3, 0, 1, 2, 1]\").logger.failures.length !== 0 // true","title":"Error recovery"},{"location":"advanced-concepts/L-attributed-grammars/","text":"Coming soon...","title":"L-attributed grammars"},{"location":"advanced-concepts/Parametrized-rules/","text":"Coming soon...","title":"Parametrized rules"},{"location":"advanced-concepts/Using-TypeScript/","text":"Pegase was coded in TypeScript and ships with its own type declarations. The types of all entities, from semantic actions to failure objects, result objects, directives, plugins, etc. can directly be imported from pegase . For a list of all available types, please refer to API > Types . Furthermore, the peg tag accepts two optional generics: the first one types the value of the resulting parser, the second types the context option. import peg, { $context, $raw, $fail } from \"pegase\"; type Context = Map<string, number>; const g = peg<number, Context>` [a-z]+ ${() => { const val = $context().get($raw()); if (!val) $fail(`Undeclared identifier \"${$raw()}\"`); else return val; }} `; // g is of type Parser<number, Context>","title":"Using TypeScript"},{"location":"advanced-concepts/Working-with-RegExp/","text":"Pegase allows for regex literals to appear directly in a peg expression. Internally, this builds an actual RegExp instance and wraps it in a RegexParser (a subclass of Parser ). At invocation, the parsing is automatically delegated to RegExp.prototype.exec . On success, the RegexParser will emit the capturing groups as children : const minutes = peg` /(\\d+):(\\d+)/ ${() => { const [hr, min] = $children(); return 60 * Number(hr) + Number(min); }} `; minutes.value(\"2:43\"); // 163 RegExp instances can also be inserted into a peg expression via tag argument: const time = /(\\d+):(\\d+)/; const minutes = peg` ${time} ${() => { const [hr, min] = $children(); return 60 * Number(hr) + Number(min); }} `; minutes.value(\"2:43\"); // 163 The RegExp 's named capturing groups (when supported by your environment) are transformed into regular Pegase captures: const date = /(?<year>\\d{4})-(?<month>\\d{2})-(?<day>\\d{2})/; const yearIs = peg` ${date} ${({ year }) => \"The year is \" + year} `; yearIs.value(\"2021-08-19\"); // \"The year is 2021\"","title":"Working with RegExp"},{"location":"advanced-concepts/Writing-a-Parser-subclass/","text":"We've seen in section Basic concepts > Building parsers that parsers are combined together to form more complex parsers. In this section, we'll go into more detail about how exactly this composition is done internally and how you could write your own Parser subclass with its own logic. Under the hood, the Parser class is derived into two categories of subclasses: Leaf classes, which don't hold a reference to other parsers. There are three of them: LiteralParser , RegexParser and CutParser . Composition classes like SequenceParser , TokenParser , PredicateParser , ActionParser , NonTerminalParser etc. which, on the contrary, reference one or more subparsers. The peg tag's role is to parse a peg expression and to generate the corresponding Parser instances. The expression 'a' | 'b' is converted by the peg tag into: new AlternativeParser([ new LiteralParser(\"a\"), new LiteralParser(\"b\") ]) &id+ is converted into: new PredicateParser( new RepetitionParser( new NonTerminalParser(\"id\"), [1, Infinity] ), true ) You get the idea. Every Parser subclass, the standard and your custom ones, must satisfy two constraints: 1) Inheriting from Parser , obviously, and 2) implementing an exec method with the following signature: exec(options: Options<Context>): Match | null; The exec method will be called when the parser is invoked . It must return null on failure and a Match object on success with the following signature: type Match = { from: number; to: number; children: any[]; } The state of the parsing process at the time of invocation is expressed by the options argument with info like the current position, the input string, the skipping state (on or off), the expected case sensitivity, etc. The exhaustive list is described in API > Types . Important : For performance reasons, this object is never recreated and always directly mutated. Log events (warning and failures) must be emitted as side-effects using the methods provided by options and options.logger . Please refer to API > Parser . Great. Once you wrote a custom Parser subclass, there are basically four options for using it, depending on your needs: You can create an explicit instance and inject it into a peg expression as a tag argument: const p = new MyParser(); const g = peg`0 | 1 | ${p}`; There is also the builder approach: const _ = data => new MyParser(data); const g = peg`0 | 1 | ${_(\"foo\")} | ${_(\"bar\")}`; If the class relies on one specific attribute that's not a number, a string, a function, a RegExp or a Parser , you can make Pegase generate instances automatically by injecting that attribute directly into the peg expression and casting it into a Parser using a plugin: peg.plugins.push({ castParser(set) { if(set instanceof Set) return new MyParser(set); } }); const g = peg`42 | ${new Set([\"a\", \"b\"])}`; You can define custom directives that generate instances of it: peg.plugins.push({ directives: { myLeafParser(_, x, y) { return new MyLeafParser(x, y); }, myCompParser(parser) { return new MyCompParser(parser); } } }); const p = peg` a: b @myCompParser b: '(' @@myLeafParser(4, 5) ')' `; If your class is a singleton, you can bind its instance to an external non-terminal: peg.plugins.push({ resolve: { myparser: new MyParser() } }); const g = peg`42 | myparser`;","title":"Writing a Parser subclass"},{"location":"advanced-concepts/Writing-a-plugin/","text":"Pegase has a plugin system that allows you to extend the base functionalities. A plugin is a simple object with four optional properties: name : the name of your plugin. castParser : A function to convert custom tag argument types into Parser instances. directives : Custom directive definitions. resolve : A string to Parser map that will be used as a fallback resolver for undefined non-terminals in your peg expressions. Think of it as global rules . For the exact type signature of these properties, please refer to API > Types . Plugins then have to be added to the peg tag's plugins array. Order matters: in case of conflict (a conflicting resolve rule, directive definition or castParser behavior), the first will win. Let's add two directives @min and @max , that transform the children of the wrapped parser to only keep respectively the minimum and the maximum value and emit it as a single child: peg.plugins.push({ name: \"my-plugin\", directives: { min: parser => peg`${parser} ${() => Math.min(...$children())}`, max: parser => peg`${parser} ${() => Math.max(...$children())}` } }); Testing it: const max = peg` list: $int+ @max $int: \\d+ @number `; max.value(\"36 12 42 3\"); // 42 To remove a plugin, you can manipulate the plugins array just like any other array: splice it, or replace it entirely: peg.plugins = peg.plugins.filter(({ name }) => name !== \"my-plugin\");","title":"Writing a plugin"},{"location":"api/Hooks/","text":"Hooks are functions that can be called from semantic actions and visitor callbacks. They provide contextual information and actions. Hook Type Availability Description $from () => Location Semantic actions, visitors Returns the start location of the current match in the input (included) $to () => Location Semantic actions, visitors Returns the end location of the current match in the input (excluded) $children () => any[] Semantic actions Returns the children produced by the current match $value () => any Semantic actions Returns the value (i.e. the single child) produced by the current match. This is undefined if there is no child, or multiple children. $raw () => string Semantic actions, visitors Returns the substring of the current match $options () => Options Semantic actions, visitors Returns the current parse options $context () => any Semantic actions, visitors Returns the parse context. Shortcut for $options().context . $warn (message: string) => void Semantic actions, visitors Emits a warning at the current match's start location $fail (message: string) => void Semantic actions, visitors Emits a semantic failure at the current match's start location. In semantic actions, this failure is only a candidate (see Failures and warnings ). $expected (expected: (string | RegExp | Expectation)[]) => void Semantic actions, visitors Emits an expectation failure at the current match's start location. In semantic actions, this failure is only a candidate and might be thrown out or merged according to the farthest failure heuristic (see Failures and warnings ). $commit () => void Semantic actions Flushes the current farthest failure to the final failure output (see Error recovery ) $emit (children: any[]) => void Semantic actions Emits the given children. $node (label: string, fields: Record<string, any>): Node Semantic actions, visitors Creates a Node with the given label and fields $visit (node: Node, visitor?: Visitor, context?: any) => any Visitors Applies the current visitor (or visitor if the second argument is provided) to node and returns the result. A new context can be passed down. $parent () => Node | null Visitors Returns the Node from which the current visit was called from, or null if the current node is root","title":"Hooks"},{"location":"api/Metagrammar/","text":"This is the peg expression of peg expressions. # Main rules: parser: grammarParser | optionsParser grammarParser: (identifier ruleParameterDefinitions directives ':' ^ optionsParser)+ optionsParser: '|'? directiveParser % '|' directiveParser: sequenceParser directives sequenceParser: minusParser+ minusParser: moduloParser % '-' moduloParser: forwardParser % ('%' repetitionRange?) forwardParser: '...'? captureParser captureParser: ('<' '...'? identifier? '>')? predicateParser predicateParser: ('&' | '!')? repetitionParser repetitionParser: primaryParser repetitionRange? primaryParser: | '.' | '$' - identifier | '\u03b5' | '^' | '(' ^ parser ')' | '>' ^ identifier '<' | '@' ^ directive | !(identifier ruleParameterDefinitions directives ':') nonTerminal | numberLiteral | stringLiteral | characterClass | escapedMeta | regexLiteral | castableTagArgument # Secondary rules: nonTerminal: identifier ruleParameters repetitionRange: | '?' | '+' | '*' | '{' value (',' value?)? '}' ruleParameterDefinitions: ('(' (identifier ('=' optionsParser)?) % ',' ')')? ruleParameters: ('(' optionsParser? % ',' ')')? directives: directive* directive: | '@' ^ identifier directiveParameters | actionTagArgument | '=>' value directiveParameters: ('(' value % ',' ')')? value: | tagArgument | stringLiteral | numberLiteral | nonTerminal | characterClass | escapedMeta | regexLiteral # Tokens: identifier @token('identifier'): /(\\$?[_a-zA-Z][_a-zA-Z0-9]*)/ numberLiteral @token('number literal'): /[0-9]+\\.?[0-9]* / stringLiteral @token('string literal'): | /'((?:[^\\\\']|\\\\.)*)'/ | /\"(?:[^\\\\\"]|\\\\.)*\"/ regexLiteral @token('regex literal'): /\\/((?:\\[[^\\]]*]|[^\\\\\\/]|\\\\.)+)\\// characterClass @token('character class'): /\\[(?:[^\\\\\\]]|\\\\.)*]/ escapedMeta @token('escaped metacharacter'): /\\\\[a-zA-Z0-9]+/ tagArgument @token('tag argument'): /~(\\d+)/ castableTagArgument @token('castable tag argument'): tagArgument actionTagArgument @token('action tag argument'): tagArgument","title":"Metagrammar"},{"location":"api/Parser/","text":"Parser<Value, Context> Class (abstract base) Property Type Description defaultOptions Partial<Options<Context>> Default parsing options. These are merged to the ones provided when calling one of the following method. parse (input: string, options?: Partial<Options<Context>>) => Result<Value, Context> Parses input and builds a Result object test (input: string, options?: Partial<Options<Context>>) => boolean Wrapper around parse . Returns the Result object's success field. value (input: string, options?: Partial<Options<Context>>) => Value Wrapper around parse . Returns the Result object's value field in case of success. Throws an Error on failure. children (input: string, options?: Partial<Options<Context>>) => any[] Wrapper around parse . Returns the Result object's children field in case of success. Throws an Error on failure. All Parser subclasses share the following properties: Property Type Description exec (options: Options<Context>) => Match | null Invokes the parser. Returns a Match on success and null on failure. LiteralParser Class (inherits from Parser ) Property Type Description Constructor new(literal: string, emit?: boolean) => LiteralParser Builds a new instance literal string The literal to be matched when the parser is invoked emit boolean Whether the parser should emit the matched substring as a single child or not RegexParser Class (inherits from Parser ) Property Type Description Constructor new(regex: RegExp) => RegexParser Builds a new instance regex RegExp The original RegExp passed to the constructor cased RegExp The RegExp used for case-sensitive matching uncased RegExp The RegExp used for case-insensitive matching","title":"Parser"},{"location":"api/Parser/#parservalue-context","text":"Class (abstract base) Property Type Description defaultOptions Partial<Options<Context>> Default parsing options. These are merged to the ones provided when calling one of the following method. parse (input: string, options?: Partial<Options<Context>>) => Result<Value, Context> Parses input and builds a Result object test (input: string, options?: Partial<Options<Context>>) => boolean Wrapper around parse . Returns the Result object's success field. value (input: string, options?: Partial<Options<Context>>) => Value Wrapper around parse . Returns the Result object's value field in case of success. Throws an Error on failure. children (input: string, options?: Partial<Options<Context>>) => any[] Wrapper around parse . Returns the Result object's children field in case of success. Throws an Error on failure. All Parser subclasses share the following properties: Property Type Description exec (options: Options<Context>) => Match | null Invokes the parser. Returns a Match on success and null on failure.","title":"Parser&lt;Value, Context&gt;"},{"location":"api/Parser/#literalparser","text":"Class (inherits from Parser ) Property Type Description Constructor new(literal: string, emit?: boolean) => LiteralParser Builds a new instance literal string The literal to be matched when the parser is invoked emit boolean Whether the parser should emit the matched substring as a single child or not","title":"LiteralParser"},{"location":"api/Parser/#regexparser","text":"Class (inherits from Parser ) Property Type Description Constructor new(regex: RegExp) => RegexParser Builds a new instance regex RegExp The original RegExp passed to the constructor cased RegExp The RegExp used for case-sensitive matching uncased RegExp The RegExp used for case-insensitive matching","title":"RegexParser"},{"location":"api/Types/","text":"Coming soon","title":"Types"},{"location":"api/Utility-functions/","text":"Coming soon","title":"Utility functions"},{"location":"api/defaultPlugin/","text":"Coming soon","title":"defaultPlugin"},{"location":"api/peg-createTag/","text":"peg Function (template tag) Property Type Description Call via template tag <Value, Context>(chunks: TemplateStringsArray, ...args: any[]) => Parser<Value, Context> Generates a Parser instance based on a peg expression trace boolean Activates tracing during peg expression parsing (called meta-parsing ) plugins Plugin[] The list of plugins attached to the tag (order matters: in case of conflicts, the first plugin wins). Can be mutated or replaced. createTag Function Property Type Description Call () => typeof peg Generates a new peg-like tag. This is useful is you need different peg tags with different plugins at the same time.","title":"peg, createTag"},{"location":"api/peg-createTag/#peg","text":"Function (template tag) Property Type Description Call via template tag <Value, Context>(chunks: TemplateStringsArray, ...args: any[]) => Parser<Value, Context> Generates a Parser instance based on a peg expression trace boolean Activates tracing during peg expression parsing (called meta-parsing ) plugins Plugin[] The list of plugins attached to the tag (order matters: in case of conflicts, the first plugin wins). Can be mutated or replaced.","title":"peg"},{"location":"api/peg-createTag/#createtag","text":"Function Property Type Description Call () => typeof peg Generates a new peg-like tag. This is useful is you need different peg tags with different plugins at the same time.","title":"createTag"},{"location":"basic-concepts/Building-parsers/","text":"Overview The peg tag accepts any valid Pegase expression and always returns a Parser instance. Pegase parsers follow the combinator paradigm: simple parsers are combined to form more complex parsers. You can read more about it in the API > Parser section. In the following table are the different expressions you can use as building blocks ( a and b representing any peg expression of higher precedence): Pegase Description Children Precedence . Matches any character [] 0 $ Matches the end of the input (equivalent to !. @token(\"end of input\") ) [] \u03b5 Matches the empty string. Equivalent to '' and always a success. [] ^ The cut operator. Always a success, commits to an alternative to prevent exploring any further in an ordered choice expression. Example : 'x' ^ a | b will not try b if 'x' was found but a failed. [] (a) Matches a Forwarded from a >id < Back reference. Matches the string literal captured as id . [] @@dir @${func} @ => 'label' etc. Syntactic sugar for directives applied to the empty literal parser: '' @dir , '' ${func} , etc. Handy if you don't care about a directive's wrapped parser. Directives generate new parsers. So children depends on whatever parser is generated. identifier Matches the non-terminal identifier Forwarded from the non-terminal identifier 'literal' Matches the string \"literal\" [] \"literal\" Matches the string \"literal\" [\"literal\"] 42 Matches the number literally (equivalent to '42' ) [] ${arg} Template tag argument ( arg is a JS expression). It can be a number (matches the number literally), a string (matches the string), a RegExp (matches the regular expression), or a Parser instance. Plugins can add support for additionnal values. If arg is a string or a number: [] . If arg is a RegExp , it emits its capturing groups (if any). If arg is a Parser instance, its children are forwarded. [a-zA-Z] Matches one character in the given character class (same syntax as RegExp character classes) [] [^a-zA-Z] Matches one character not in the given character class (same syntax as RegExp negated character classes) [] \\n \\s \\xAF \\uA6F1 etc. Matches the escaped metacharacter as a RegExp expression (i.e. \\s matches any whitespace, \\S any non-whitespace, \\uA6F1 matches the unicode character A6F1 , etc. ( See RegExp documentation for a complete list of supported metacharacters). [] /ab/ /\\w+/ /(\\d+):(\\d+)/ etc. Matches the regex (same syntax as JS' RegExp literals) The regex's capturing groups a? Matches zero or one a Forwarded from a 1 a+ Matches one or more a Forwarded and concatenated from a a* Matches zero or more a Forwarded and concatenated from a a{4} Matches a exactly 4 times. Please note that quantifiers can be parametrized by tag argument: a{${n}} , where n is a JS expression. Forwarded and concatenated from a a{4, 15} Matches a between 4 and 15 times Forwarded and concatenated from a a{4,} Matches a at least 4 times Forwarded and concatenated from a &a Matches a without consuming any input [] 2 !a Succeeds if a fails and vice-versa, doesn't consume input [] <id>a If a emits a single child (called value of a ), it's captured and assigned to the identifier \"id\" , which can then be used in semantic actions and back references. Otherwise, \"id\" will be set to undefined . Forwarded from a 3 <>id Shortcut for <id>id , where id is a non-terminal Forwarded from the non-terminal id <...id>a Captures the children of a and assigns them to the identifier \"id\" as an array Forwarded from a <...>id Shortcut for <...id>id , where id is a non-terminal Forwarded from the non-terminal id ...a Skips input character by character until a is matched. This can be used to implement synchronization to recover from errors and is equivalent to (!a .)* a . Write ...&a if you want to sync to a without consuming a . See Failure recovery . Forwarded from a 4 a % b a %? b a %{3} b a %{3,} b etc. Matches a sequence of a separated by b . The % operator can be parametrized using the quantifiers described above. a % b is equivalent to a (b a)* , a %? b to a (b a)? , etc. Forwarded and concatenated from the matched sequence of a and b 5 a - b Matches a but not b (fails if b succeeds). Equivalent to !b a . Forwarded from a 6 a b Matches a followed by b Forwarded and concatenated from a and b 7 a @dir a @dir(x, y) a @dir(x, ${y}) a @dir @other etc. Applies the directive(s) to the parser a . Directives are functions that take a parser and return a new parser. They can take additional arguments and can be chained. Directives generate new parsers. So children depends on whatever parser is generated. 8 a ${func} Semantic action. func is a JS function passed as tag argument. It will be called if a succeeds and will receive a 's captures as a single object argument. This is in fact a shortcut for a @action(${func}) and can thus be chained with other directives as described above. [<return value of func>] if that value is different than undefined , otherwise forwarded from a a => 'label' Shortcut for a @node('label') . It will generate a Pegase node labeled \"label\" and emit it as a single child. See AST and visitors . The label value can be inserted via tag argument: a => ${str} , where str is a JS string. [<the generated node>] a | b Ordered choice. Succeeds if a or b succeeds (order matters). Please note that you can add a leading bar for aesthetic purposes. Forwarded from a or b 9 id: a $id: a id @dir: a id(u, v): a id(p = \\d): a etc. Rule. This creates a rule as an alias to parser a . Rules can be stacked to form grammars . They can also be parametrized and the parameters can take default values. If directives are specified just before : , they are applied to the whole right-side expression a . Adding $ at the beginning of a rule name applies an implicit @token directive (the display name in failure reports will be the rule name transformed to space case, i.e. $myToken: a is equivalent to $myToken @token(\"my token\"): a ). Forwarded from the topmost rule 10 Any character . Children : [] Precedence : 0 Matches any character. End of input $ Children : [] Precedence : 0 Matches the end of the input. This expression is syntactic sugar for !. @token(\"end of input\") . Indeed, the end of the input has been reached when there's no character left, thus when \"not any character\" ( !. ) can be evaluated as true. Epsilon \u03b5 Children : [] Precedence : 0 Matches the empty string. Strictly equivalent to '' and always a success. Cut operator ^ Children : [] Precedence : 0 In an ordered choice expression, the cut operator commits to the current alternative to prevent exploring any further in case it fails. Example : 'x' ^ a | b will not try b if 'x' was found but a failed. Although it's called an operator , it's really a Parser on its own. Used outside an ordered choice expression, it's a no-op. Read more in Cut operator . Parenthesis (a) Children : Forwarded from a Precedence : 0 Delegates the parsing to any peg subexpression a . Back reference >id< Children : [] Precedence : 0 Back reference to an earlier string capture. This will matches the string literal captured as id . Read more in Semantic action and dataflow .","title":"Building parsers"},{"location":"basic-concepts/Building-parsers/#overview","text":"The peg tag accepts any valid Pegase expression and always returns a Parser instance. Pegase parsers follow the combinator paradigm: simple parsers are combined to form more complex parsers. You can read more about it in the API > Parser section. In the following table are the different expressions you can use as building blocks ( a and b representing any peg expression of higher precedence): Pegase Description Children Precedence . Matches any character [] 0 $ Matches the end of the input (equivalent to !. @token(\"end of input\") ) [] \u03b5 Matches the empty string. Equivalent to '' and always a success. [] ^ The cut operator. Always a success, commits to an alternative to prevent exploring any further in an ordered choice expression. Example : 'x' ^ a | b will not try b if 'x' was found but a failed. [] (a) Matches a Forwarded from a >id < Back reference. Matches the string literal captured as id . [] @@dir @${func} @ => 'label' etc. Syntactic sugar for directives applied to the empty literal parser: '' @dir , '' ${func} , etc. Handy if you don't care about a directive's wrapped parser. Directives generate new parsers. So children depends on whatever parser is generated. identifier Matches the non-terminal identifier Forwarded from the non-terminal identifier 'literal' Matches the string \"literal\" [] \"literal\" Matches the string \"literal\" [\"literal\"] 42 Matches the number literally (equivalent to '42' ) [] ${arg} Template tag argument ( arg is a JS expression). It can be a number (matches the number literally), a string (matches the string), a RegExp (matches the regular expression), or a Parser instance. Plugins can add support for additionnal values. If arg is a string or a number: [] . If arg is a RegExp , it emits its capturing groups (if any). If arg is a Parser instance, its children are forwarded. [a-zA-Z] Matches one character in the given character class (same syntax as RegExp character classes) [] [^a-zA-Z] Matches one character not in the given character class (same syntax as RegExp negated character classes) [] \\n \\s \\xAF \\uA6F1 etc. Matches the escaped metacharacter as a RegExp expression (i.e. \\s matches any whitespace, \\S any non-whitespace, \\uA6F1 matches the unicode character A6F1 , etc. ( See RegExp documentation for a complete list of supported metacharacters). [] /ab/ /\\w+/ /(\\d+):(\\d+)/ etc. Matches the regex (same syntax as JS' RegExp literals) The regex's capturing groups a? Matches zero or one a Forwarded from a 1 a+ Matches one or more a Forwarded and concatenated from a a* Matches zero or more a Forwarded and concatenated from a a{4} Matches a exactly 4 times. Please note that quantifiers can be parametrized by tag argument: a{${n}} , where n is a JS expression. Forwarded and concatenated from a a{4, 15} Matches a between 4 and 15 times Forwarded and concatenated from a a{4,} Matches a at least 4 times Forwarded and concatenated from a &a Matches a without consuming any input [] 2 !a Succeeds if a fails and vice-versa, doesn't consume input [] <id>a If a emits a single child (called value of a ), it's captured and assigned to the identifier \"id\" , which can then be used in semantic actions and back references. Otherwise, \"id\" will be set to undefined . Forwarded from a 3 <>id Shortcut for <id>id , where id is a non-terminal Forwarded from the non-terminal id <...id>a Captures the children of a and assigns them to the identifier \"id\" as an array Forwarded from a <...>id Shortcut for <...id>id , where id is a non-terminal Forwarded from the non-terminal id ...a Skips input character by character until a is matched. This can be used to implement synchronization to recover from errors and is equivalent to (!a .)* a . Write ...&a if you want to sync to a without consuming a . See Failure recovery . Forwarded from a 4 a % b a %? b a %{3} b a %{3,} b etc. Matches a sequence of a separated by b . The % operator can be parametrized using the quantifiers described above. a % b is equivalent to a (b a)* , a %? b to a (b a)? , etc. Forwarded and concatenated from the matched sequence of a and b 5 a - b Matches a but not b (fails if b succeeds). Equivalent to !b a . Forwarded from a 6 a b Matches a followed by b Forwarded and concatenated from a and b 7 a @dir a @dir(x, y) a @dir(x, ${y}) a @dir @other etc. Applies the directive(s) to the parser a . Directives are functions that take a parser and return a new parser. They can take additional arguments and can be chained. Directives generate new parsers. So children depends on whatever parser is generated. 8 a ${func} Semantic action. func is a JS function passed as tag argument. It will be called if a succeeds and will receive a 's captures as a single object argument. This is in fact a shortcut for a @action(${func}) and can thus be chained with other directives as described above. [<return value of func>] if that value is different than undefined , otherwise forwarded from a a => 'label' Shortcut for a @node('label') . It will generate a Pegase node labeled \"label\" and emit it as a single child. See AST and visitors . The label value can be inserted via tag argument: a => ${str} , where str is a JS string. [<the generated node>] a | b Ordered choice. Succeeds if a or b succeeds (order matters). Please note that you can add a leading bar for aesthetic purposes. Forwarded from a or b 9 id: a $id: a id @dir: a id(u, v): a id(p = \\d): a etc. Rule. This creates a rule as an alias to parser a . Rules can be stacked to form grammars . They can also be parametrized and the parameters can take default values. If directives are specified just before : , they are applied to the whole right-side expression a . Adding $ at the beginning of a rule name applies an implicit @token directive (the display name in failure reports will be the rule name transformed to space case, i.e. $myToken: a is equivalent to $myToken @token(\"my token\"): a ). Forwarded from the topmost rule 10","title":"Overview"},{"location":"basic-concepts/Building-parsers/#any-character","text":". Children : [] Precedence : 0 Matches any character.","title":"Any character"},{"location":"basic-concepts/Building-parsers/#end-of-input","text":"$ Children : [] Precedence : 0 Matches the end of the input. This expression is syntactic sugar for !. @token(\"end of input\") . Indeed, the end of the input has been reached when there's no character left, thus when \"not any character\" ( !. ) can be evaluated as true.","title":"End of input"},{"location":"basic-concepts/Building-parsers/#epsilon","text":"\u03b5 Children : [] Precedence : 0 Matches the empty string. Strictly equivalent to '' and always a success.","title":"Epsilon"},{"location":"basic-concepts/Building-parsers/#cut-operator","text":"^ Children : [] Precedence : 0 In an ordered choice expression, the cut operator commits to the current alternative to prevent exploring any further in case it fails. Example : 'x' ^ a | b will not try b if 'x' was found but a failed. Although it's called an operator , it's really a Parser on its own. Used outside an ordered choice expression, it's a no-op. Read more in Cut operator .","title":"Cut operator"},{"location":"basic-concepts/Building-parsers/#parenthesis","text":"(a) Children : Forwarded from a Precedence : 0 Delegates the parsing to any peg subexpression a .","title":"Parenthesis"},{"location":"basic-concepts/Building-parsers/#back-reference","text":">id< Children : [] Precedence : 0 Back reference to an earlier string capture. This will matches the string literal captured as id . Read more in Semantic action and dataflow .","title":"Back reference"},{"location":"basic-concepts/Directives/","text":"Directives are functions defined in plugins with the following signature: type Directive = (parser: Parser, ...args: any[]) => Parser They transform a Parser into a new Parser . In peg expressions, they are applied to sub-expressions, either explicitly via the @ notation or implicitly with syntactic sugar. Such application triggers the peg tag to call the function while generating the resulting Parser . The first parser argument is the parser the directive is wrapped around. The args rest arguments are the additional arguments passed to the directive with the optional bracketed argument syntax (these arguments can include tag arguments). The resulting Parser is the return value of the function. To demonstrate that, we're going to add a @toNumber directive to the plugin chain. This directive should apply a semantic action to the wrapped parser to cast the parsed substring into a number and emit that number as a single child: peg.plugins.push({ directives: { toNumber: parser => peg`${parser} ${() => Number($raw())}` } }); Please refer to the Writing a plugin section for more info about plugins. Now, instead of copy/pasting the semantic action every time you need to convert a portion of the input into a number, you can simply apply the directive: const p = peg` num: $int @toNumber $int: \\d+ `; p.children(\"23\"); // [23] p.value(\"65\"); // 65 Directives can be chained. Let's add a toBase directive that converts an emitted number into a string, given a custom base: peg.plugins.push({ directives: { toNumber: parser => peg`${parser} ${() => Number($raw())}`, toBase: (parser, base) => peg`${parser} ${() => $value().toString(base)}` } }); const p = peg` num: $int @toNumber @toBase(16) $int: \\d+ `; p.value(\"15\"); // \"f\" p.value(\"3153\"); // \"c51\" Directives are used for a wide range of purposes, from wrapping parsers in tokens, making some semantic behavior quickly reusable, toggling whitespace skipping, etc. There are a bunch of standard directives defined by default, like @omit , @raw , @number , @token , @reverse , etc. See API > defaultPlugin for more info. In fact, the syntax of semantic actions a ${func} is syntactic sugar for a @action(${func}) . The standard @action directive takes in a Parser and a function and returns an instance of ActionParser . In specific cases, you might not care about the wrapped parser. This is for example the case for the @commit directive whose sole role is to commit the current farthest failure (see Advanced concepts > Error recovery ). In such pure side-effect situations, the idea is to wrap the directive around an empty literal (always succeeds): a ('' @commit) b But instead of writing it explicitly, there is syntactic sugar too: adding another @ in front of the directive. The following expression is thus equivalent to the previous one: a @@commit b Please note that this also works for implicit directives like semantic actions: const p = peg` \"a\" @${() => \"b\"} \"c\" `; p.children(\"ac\"); // [\"a\", \"b\", \"c\"]","title":"Directives"},{"location":"basic-concepts/Failures-and-warnings/","text":"Producing accurate error messages is notoriously difficult when it comes to PEG parsing. That's because when an input error triggers a parse failure, the parser backtracks to all the parent alternatives, tries them out, fails repetitively , before ultimately exiting with an error. Thus, failures are being emitted way after the one that's relevant. So which one should be displayed ? You also can't just short-exit on the first failure you encounter, since that would prohibit any backtracking and defeat the purpose of PEGs. Because of PEG's backtracking nature, a parse failure isn't necessarily an input error . This is well explained in this paper for those who want to dive deeper into the subject matter. Other parsing algorithms like LL or LALR don't suffer from this problem but are also more difficult to implement and more restrictive in the type of parsing expressions they allow. Fortunately for us, there exists a way out of this. As we've just established, the main problem is to be able to \"rank\" failures and \"guess\" which ones are more relevant. By the very design of PEGs, this can never be an exact science and one has to use an approximative method, called heuristic , to produce good enough results. Pegase implements the farthest failure heuristic , which considers the farthest failure(s) in terms of input position to be the most relevant. The general idea is that a failure emitted at input position n will generally be more relevant than a failure emitted at position n - x , where x is a positive integer, because x more characters have been successfully recognized by the parser at that point. Failures and warnings (called log events ) are tracked at parse time. warnings and failures are then attached to the parse result as arrays, whether the match fails or succeeds (a successful match can produce failures, see Advanced concepts > Error recovery ). In Pegase, there are two types of failures : Expectation failures These are automatically emitted when a literal, a regexp or a token mismatched, or if a portion of the input matched where it should not have (cf. negative predicates ( !a )). const g = peg`'a' ('b' | 'c' | 'd' @token(\"the awesome letter d\") | ![b-e] .)`; g.parse('ae').logger.toString() (1:2) Failure: Expected \"b\", \"c\", the awesome letter d or mismatch of \"e\" > 1 | ae | ^ You can also manually emit them in semantic actions using the $expected hook (please note that this will override any failure emitted from inside the peg expression the action is wrapped around): const g = peg`'a' ('b' | . ${() => { if (![\"c\", \"d\"].includes($raw())) $expected([\"c\", \"d\"]); }})`; g.parse(\"ae\").logger.toString() (1:2) Failure: Expected \"b\", \"c\" or \"d\" > 1 | ae | ^ Semantic failures These are emitted by calling the $fail hook from a semantic action. They're useful when dealing with errors that can not be expressed as missing terminals, like undeclared identifiers, type errors, break statements outside of loops, etc. Such errors will also override any failure emitted from inside the peg expression the action is wrapped around. They don't terminate the parser directly either and can thus act as backtracking instructions. const g = peg`[a-z]+ ${() => { const val = $context().get($raw()); if (!val) $fail(`Undeclared identifier \"${$raw()}\"`); else return val; }}`; const context = new Map([[\"foo\", 42], [\"bar\", 18]]); g.value(\"foo\", { context }) 42 g.parse(\"baz\", { context }).logger.toString() (1:1) Failure: Undeclared identifier \"baz\" > 1 | baz | ^ If there are several failures at the farthest position n , they are folded into one with the following logic: If they're only expectation failures, the expectations are merged as illustrated above. If there is a semantic failure, it will override all other failures. In case of multiple semantic failures at the same position, the last one will win. If you want to identify multiple input errors at once, you have to do error recovery . This is done using failure commits and synchronization expressions ( ...a ). See Advanced concepts > Error recovery for more info. Warnings can be emitted in semantic actions using the $warn hook . They are collected in a side-effect manner and don't influence the parsing process: const p = peg` declaration: 'class' (identifier ${() => { if (!/^[A-Z]/.test($raw())) $warn(\"Class names should be capitalized\"); }}) '{' '}' $identifier: [a-zA-Z]+ `; p.parse(\"class test {\").logger.toString() (1:7) Warning: Class names should be capitalized > 1 | class test { | ^ (1:13) Failure: Expected \"}\" > 1 | class test { | ^ If you want to do more elaborated stuff than to simply pretty-print the logs, like processing them programmatically, you have direct access using the warnings and failures properties on the result object. These are just arrays of objects describing the log events. Please see API > Parser for more details. Warnings and failures can also be emitted during AST visits. See Advanced concepts > AST and visitors .","title":"Failures and warnings"},{"location":"basic-concepts/Handling-whitespaces/","text":"When it comes to parsing, whitespaces are usually an annoying part to handle. Well, not with Pegase which provides you with a set of default behaviors and options to make everything straightforward. For most use cases, you won't even have to think about it. By default, whitespaces are skipped before every terminal parser. Terminal parsers include: Literal parsers (like \"lit\" , 'lit' , 42 or \u03b5 ) Regexp parsers (like [a-z] , \\w , . or ${/my_js_regexp/} ) Token parsers, including the end-of-input token $ and every parser wrapped with the @token directive (we will go to that in the next section ). This behavior can be changed. All Parser 's methods ( parse , test , value and children ) actually accept an optional second argument, an options object. These are the parse options, two of which are of interest with the matter at hand here: skipper , a Parser instance that should match the substring you want to skip before every terminal. When you don't provide that option, a default Parser is used which skips any sequence of \\s . skip , a boolean value that enables or disables skipping ( true by default). In the following example, default options are used. Whitespaces are skipped before each 'a' and before the implicit token $ (set option complete to false to avoid having an implicit $ at the end of your peg expression): const g = peg`'a'+`; g.test(\" aa a a a a \"); // true Next, let's disable skipping entirely: const g = peg`'a'+`; g.test(\" aa a a a a \", { skip: false }); // false g.test(\"aaaaaa\", { skip: false }); // true You can toggle skipping for specific parts of your peg expression by using the @skip and/or @noskip directives: const g = peg`('a'+ @noskip) 'b'`; g.test(\" aa a a a a b\"); // false g.test(\"aaaaaaa b\"); // true If none of these options suits your needs, you can use explicit whitespaces and disable auto-skipping once and for all: const g = peg` array: '[' _ '1' % ',' _ ']' _: \\s+ `; g.defaultOptions.skip = false; g.test(\"[1,1,1]\"); // false g.test(\"[ 1,1,1 ]\"); // true g.test(\"[ 1, 1,1 ]\"); // false","title":"Handling whitespaces"},{"location":"basic-concepts/Semantic-action-and-dataflow/","text":"Differentiating between faulty and correct inputs is generally only part of the job we expect from a parser. Another big part is to run routines and generate data as a side-effect. In this section, we'll talk semantic actions , dataflow , parse children and captures . PEG parsers are top-down parsers, meaning the peg expressions are recursively invoked in a depth-first manner, guided by a left-to-right input read. This process can be represented as a tree, called concrete syntax tree. Let's illustrate that with the following grammar: const prefix = peg` expr: op expr expr | \\d op: '+' | '-' | '*' | '/' `; The input \"+ 5 * 2 6\" would generate the following syntax tree: Pegase implements a mechanism by which every individual parser can emit an array of values called children . For example, in the op rule, '+' is a parser in and of itself who will succeed if a plus character can be read from the input. It's called a literal parser. You can make any literal parser emit the substring it matched as a single child by using double quotes instead of single quotes. More generally, you can make any parser emit the substring it matched as a single child by using the @raw directive. For example, \\d @raw will emit the exact digit character it matched, i.e. the input 5 would produce [\"5\"] as children . children can be collected and processed in parent parsers through composition . Some composition patterns process children automatically. This is for example the case with the sequence expression op expr expr : The children of that sequence is the concatenation of the individual children of op , expr and expr . Please refer to the table in Building parsers , column Children , for more information. We can also customize that processing behavior with the help of semantic actions as we'll discuss in a second. For now, let's rewrite the grammar to make it emit the operators and the digits it matched: const prefix = peg` expr: op expr expr | \\d @raw op: \"+\" | \"-\" | \"*\" | \"/\" `; Now children are emitted and propagated during the parsing process: Indeed: prefix.parse(\"+ 5 * 2 6\").children; // [\"+\", \"5\", \"*\", \"2\", \"6\"] prefix.children(\"+ 5 * 2 6\"); // [\"+\", \"5\", \"*\", \"2\", \"6\"] That can already be pretty useful, but what you usually want to do is to process these children in certain ways at strategic steps during parse time in order to incrementally build your desired output. This is where semantic actions come into play. A semantic action wraps around a Parser and calls a callback on success. If it returns undefined , children will be forwarded. Any other return value will be emitted as a single child. Let's take our prefix grammar and say we want to make it generate the input expression in postfix notation (operators after operands). All we need to do is wrap a semantic action around op expr expr , reorder its children to postfix order, join them into a string and emit that string as a single child. import peg, { $children } from \"pegase\"; const prefix = peg` expr: | op expr expr ${() => { const [op, a, b] = $children(); return [a, b, op].join(\" \"); }} | \\d @raw op: \"+\" | \"-\" | \"*\" | \"/\" `; $children is a hook. Hooks are global functions that provide contextual information and operations in semantic actions , like reading children , emitting warnings or failures, getting the current position, etc. Please refer to the Hooks section for a list of all available hooks. Recursively, this process will transform the entire input from prefix to postfix: Let's test this: prefix.children(\"+ 5 * 2 6\"); // [\"5 2 6 * +\"] When a Parser emits only a single child, it's called the value of that Parser . prefix.parse(\"+ 5 * 2 6\").value; // \"5 2 6 * +\" prefix.value(\"+ 5 * 2 6\"); // \"5 2 6 * +\" value is undefined if there is no child, or multiple children. You can quickly convince yourself that the prefix grammar can only ever return one child. Thus, except in case of a parse failure, there is always a value to be read from prefix . A well designed peg expression should always have easily predictable children for itself and all its nested parser parts. A quick glimpse at a grammar should always give you a good general picture of the dataflow. In most cases, a good design choice is to ensure that non-terminals always emit only up to one child but that's ultimately up to you. Great, but at the end of the day children are just unlabeled propagated values. Sometimes that's what you want (typically when you're parsing list-ish data: a list of phone numbers, a list of operators and operands, a list of arguments to a function, etc.), but very often in semantic actions, you want to be able to grab a specific parser's value by name. This is where captures will come in handy. A capture expression <id>a binds the value (the single child) of parser a to the identifier id , which can be used in semantic actions. Two things to keep in mind: If a is a non-terminal id and you want to bind its value to its own name, you can simply write <>id (equivalent to <id>id ). Captures are passed as a semantic action's first and unique argument. Taking this into consideration, our prefix-to-postfix converter can be rewritten in a slightly nicer way: const prefix = peg` expr: | <>op <a>expr <b>expr ${({ op, a, b }) => [a, b, op].join(' ')} | \\d @raw op: \"+\" | \"-\" | \"*\" | \"/\" `; As an exercise, try to rewrite the prefix grammar so that its value is the actual result of the calculation. What if you want to emit more than one child, no child at all, or [undefined] from a semantic action ? This has to be done explicitly by calling the $emit hook which takes a custom children array as an argument: import peg, { $emit } from \"pegase\"; peg`a ${() => {}}`; // forwards a's children (pass-through) peg`a ${() => undefined}`; // forwards a's children (pass-through) peg`a ${() => $emit([undefined])}`; // emits a single child (undefined) peg`a ${() => 5}`; // emits a single child (5) peg`a ${() => $emit([])}`; // emits no child peg`a ${() => $emit([1, true, 2])}`; // emits multiple children","title":"Semantic action and dataflow"},{"location":"basic-concepts/Tokens/","text":"To understand the need for a token concept, let's take a look at a quick example. Let's try and write a grammar to match and extract a coma-separated integer list: const intList = peg` list: integer % ',' integer @raw: \\d+ `; But this doesn't work. Indeed, as whitespace skipping happens before every \\d , \\d+ can match any space-separated digit list. Thus, \"23 4, 45\" would be a valid input because 23 4 would be considered one integer: intList.children(\"23 4, 45\"); // [\"23 4\", \"45\"] You might intuitively want to disable skipping for the integer rule: const intList = peg` list: integer % ',' integer @raw @noskip: \\d+ `; But this doesn't work either, because now you don't allow for whitespaces before integers. So a simple \"1 , 1\" would fail when it should not: intList.parse(\"1 , 1\").logger.toString() (1:4) Failure: Expected /\\d/ > 1 | 1 , 1 | ^ If you think about it, what we need is to skip whitespaces right before integer but not inside it. Something like \\d (\\d* @noskip) but without the repetitiveness. And that's exactly what a token parser does: A token parser wraps around a Parser and performs pre-skipping before invoking it. Skipping is then disabled inside. Essentially, the token parser avoids the need for explicit whitespaces in the grammar and for an external tokenizer by allowing you to treat any arbitrary peg expression as if it were a terminal. Let's try it out and see that it works as expected: const intList = peg` list: integer % ',' integer @raw @token: \\d+ `; intList.parse(\"23 4, 45\").logger.toString() (1:4) Failure: Expected \",\" or end of input > 1 | 23 4, 45 | ^ A token can be given a display name to improve failure logging. Tokens often have a lexeme semantic, meaning we want to label them with names and don't much care about their internal syntactical details. This is indeed what happens with external tokenizers. It can be done with Pegase by passing a string as an argument to the @token directive: const intList = peg` list: integer % ',' integer @raw @token(\"fancy integer\"): \\d+ `; intList.parse(\"12, \").logger.toString() (1:5) Failure: Expected fancy integer > 1 | 12, | ^ The $id shortcut : The pattern that will appear the most is probably fancyToken @token(\"fancy token\") , there will likely be some repetition between the rule name and the display name. That's why Pegase has a shortcut for it: by starting your rule name with a dollar sign, an implicit @token directive is added whose display name is inferred by transforming PascalCase, camelCase and snake_case rule names to space case: peg`$lowerCaseWord: [a-z]+`; // is equivalent, without the sugar, to peg`$lowerCaseWord @token(\"lower case word\"): [a-z]+`; peg`$two_digit_integer: \\d{2}`; // is equivalent to peg`$two_digit_integer @token(\"two digit integer\"): \\d{2}`;","title":"Tokens"},{"location":"more/Bug-report-and-discussion/","text":"If you encounter a bug, or have a suggestion or a question, feel free to contact me via the GitHub issue page .","title":"Bug report and discussion"},{"location":"more/Ideas/","text":"Here are some features that I strongly consider adding in future releases of Pegase. If they are listed here, it's mainly because I'm not so sure if they are relevant in real use cases, so I'll basically wait and see if there is enough demand before bloating the library with unused gadgets. If you're interested in one of these features, please let me know on GitHub . Fatal failures . Whether they're emitted in semantic actions or visitors, failures are emitted as side-effects and don't stop the current process. You could stop everything by throwing an exception, sure. But maybe fatal failures should be treated as first-class citizens. It's unclear whether Pegase should catch exceptions internally and transform them into top-priority semantic failures before short-exiting, or if emitting failures and short-exiting should remain two separate things. Cuts for repetitions . The idea would be to have a cut operator, similar to ^ , but to break repetitions (like a+ , a* , etc.). A sort of break statement for peg expressions. Peg repetitions are greedy by nature, but this might not always be what you want. Default values for captures . The syntax would be something like <id = 0>a , <id = ${expr}>a (where expr is a JS expression), etc. The default value would be captured as id if a 's value is undefined . Semantic predicates .","title":"Ideas"}]}